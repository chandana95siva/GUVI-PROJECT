{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SUPERVISED LEARNING"
      ],
      "metadata": {
        "id": "zpHJDEsZE310"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dependent Variable (Target): Weekly_Sales\n",
        "# Independent Variables : (Features): Store,Type,Size,Dept,Date,IsHoliday,Temperature,Fuel_Price,MarkDown1 to MarkDown5,CPI,Unemployment"
      ],
      "metadata": {
        "id": "M0p6VirIn7A6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# READING CSV FILE:"
      ],
      "metadata": {
        "id": "d35cD0v-ob-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "hML8Qf63pH2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store = pd.read_csv('/content/stores_data_set.csv')"
      ],
      "metadata": {
        "id": "7Dr84t7Sn7S6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales = pd.read_csv('/content/sales_data_set.csv')"
      ],
      "metadata": {
        "id": "_s4EQ94Donnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = pd.read_csv('/content/Features_data_set.csv')"
      ],
      "metadata": {
        "id": "Z4xl9u89onyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store.columns"
      ],
      "metadata": {
        "id": "h58BswNZpNQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales.columns"
      ],
      "metadata": {
        "id": "Nx_-uUq8ptvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features.columns"
      ],
      "metadata": {
        "id": "kzv8o8-SpwVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store.shape"
      ],
      "metadata": {
        "id": "R5JJiMFkp2jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales.shape"
      ],
      "metadata": {
        "id": "w43fWJs5p7Zj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features.shape"
      ],
      "metadata": {
        "id": "cVYQgCHap9BX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge the DataFrames on common columns\n",
        "df = pd.merge(store, sales, on=['Store'], how='inner')\n",
        "df = pd.merge(df, features, on=['Store', 'Date'], how='inner')"
      ],
      "metadata": {
        "id": "87bVyM4AqEjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "h7BYPmFuqbmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning"
      ],
      "metadata": {
        "id": "mnkH_daErfoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the redundant column 'IsHoliday_y'\n",
        "df.drop('IsHoliday_y', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "RyT9Gtnsqfhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.rename(columns={'IsHoliday_x': 'IsHoliday'}, inplace=True)"
      ],
      "metadata": {
        "id": "6L7wzpn-qq0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "1t_nq_bequ4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking data type\n",
        "df.dtypes"
      ],
      "metadata": {
        "id": "IvIRuXmhquVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'Date' column to datetime format\n",
        "\n",
        "df['Date']  = pd.to_datetime(df['Date'], format='%d/%m/%Y', errors='coerce')"
      ],
      "metadata": {
        "id": "UPmD3nVQrQ3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 'IsHoliday' column will have values of 1 where it was True and 0 where it was False.\n",
        "df['IsHoliday'] = df['IsHoliday'].astype(int)"
      ],
      "metadata": {
        "id": "roULdozxrlPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Categorical Feature Counts:\n",
        "categorical_counts = df['Type'].value_counts()\n",
        "print(categorical_counts)"
      ],
      "metadata": {
        "id": "LW421x0drpa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping 'Type' categories to numeric values\n",
        "type_mapping = {'A': 1, 'B': 2, 'C': 3}\n",
        "df['Type'] = df['Type'].map(type_mapping)"
      ],
      "metadata": {
        "id": "mkFKVrA4sk-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "id": "t22x91NhslOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handling Missing Values:"
      ],
      "metadata": {
        "id": "V4evWrhqv9RN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the total number of missing values in each column\n",
        "missing_values = df.isnull().sum()\n",
        "\n",
        "# Calculate the percentage of missing values in each column\n",
        "percentage_missing = (missing_values / len(df)) * 100\n",
        "\n",
        "# Create a DataFrame to display the results\n",
        "missing_data = pd.DataFrame({'Missing Values': missing_values, 'Percentage': percentage_missing})\n",
        "\n",
        "print(missing_data)\n"
      ],
      "metadata": {
        "id": "jJJc7cq0m0Co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Markdown_columns  ---> Markdown refers to promotional discounts or reductions in prices.(weekly deduction for all dept)\n",
        "# Markup ---> (price increased based on demand)refers to the difference between the cost of a product or service and its selling price.(weekly)"
      ],
      "metadata": {
        "id": "ys4tmlXVv2LM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filling missing values with zeros,because there is no markdown or markup given by store for that particular week\n",
        "df[['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']] = df[['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']].fillna(0)"
      ],
      "metadata": {
        "id": "suoU6vq6v2Ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "8fWL0FStv2Ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# De_duplication"
      ],
      "metadata": {
        "id": "LW9LwcFmwcMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = df.duplicated().sum()"
      ],
      "metadata": {
        "id": "DbZ7ik0nv1-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total Duplicated datapoints:\",a)"
      ],
      "metadata": {
        "id": "55_GzSv-v1Z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXPLORATORY DATA ANALYSIS(EDA):"
      ],
      "metadata": {
        "id": "dk3Y0kYqwsMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "4-SXWDLRwr15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary Statistics:\n",
        "summary_stats = df.describe()\n",
        "print(summary_stats)"
      ],
      "metadata": {
        "id": "uDa9877gwofI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Boxplot for Weekly Sales by Store Type\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot(x='Type', y='Weekly_Sales', data=df)\n",
        "plt.xlabel('Store Type')\n",
        "plt.ylabel('Weekly Sales')\n",
        "plt.title('Weekly Sales Distribution by Store Type')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "PS5mUTa2OD54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# Countplot for Store Types\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(x='Type', data=df)\n",
        "plt.xlabel('Store Type')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of Store Types')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FVPVPSQOODnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Group by 'Date' and calculate the sum of 'Weekly_Sales'\n",
        "weekly_sales_over_time = df.groupby('Date')['Weekly_Sales'].sum()\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(weekly_sales_over_time, marker='o', linestyle='-', color='b')\n",
        "plt.title('Weekly Sales Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Weekly Sales')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mKCxoSAAMK7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap:\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming df is your DataFrame\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "# Set up the matplotlib figure\n",
        "plt.figure(figsize=(12, 10))\n",
        "\n",
        "# Create a heatmap with annotations\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "\n",
        "# Set the title of the plot\n",
        "plt.title('Correlation Heatmap')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n",
        "# Print the correlation values\n",
        "print(\"\\nCorrelation Matrix:\")\n",
        "print(correlation_matrix)\n"
      ],
      "metadata": {
        "id": "4BXL7rIswoTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# There is a positive correlation between 'Size' and 'Weekly_Sales', indicating that larger stores tend to have higher weekly sales.\n",
        "# IsHoliday' shows a slight positive correlation with 'MarkDown3', 'MarkDown4', and 'MarkDown5', suggesting that markdowns might be more prevalent during holidays\n",
        "# 'Unemployment' has a negative correlation with 'CPI', indicating a potential relationship between unemployment rates and consumer price index."
      ],
      "metadata": {
        "id": "cej2UgvcwoFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering:"
      ],
      "metadata": {
        "id": "LuS4kUSmFqU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate 'Markdown_Total' by summing up all Markdowns\n",
        "df['Markdown_Total'] = df[['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']].sum(axis=1)"
      ],
      "metadata": {
        "id": "ivqWKq3CReZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature engineering for date-related features\n",
        "df['Day'] = df['Date'].dt.day\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Year'] = df['Date'].dt.year"
      ],
      "metadata": {
        "id": "5jPG4VuWFd3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the original 'Date' column\n",
        "df = df.drop('Date', axis=1)"
      ],
      "metadata": {
        "id": "GrI_rnd3Fdwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns=['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5'], inplace=True)"
      ],
      "metadata": {
        "id": "TxEaEG8HPaLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "Ct2YM89jFdnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe().T"
      ],
      "metadata": {
        "id": "BtCSA_YNNPOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set the style of seaborn\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Histograms of numerical features\n",
        "numerical_features = ['Weekly_Sales', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'Markdown_Total']\n",
        "df[numerical_features].hist(bins=20, figsize=(15, 10), layout=(2, 3))\n",
        "plt.suptitle('Histograms of Numerical Features', fontsize=16)\n",
        "plt.show()\n",
        "\n",
        "# Box plots of numerical features\n",
        "plt.figure(figsize=(15, 6))\n",
        "sns.boxplot(data=df[numerical_features])\n",
        "plt.title('Box Plots of Numerical Features', fontsize=16)\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Scatter plots of numerical features against Weekly_Sales\n",
        "plt.figure(figsize=(15, 6))\n",
        "for i, feature in enumerate(['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'Markdown_Total'], start=1):\n",
        "    plt.subplot(2, 3, i)\n",
        "    sns.scatterplot(x=feature, y='Weekly_Sales', data=df, alpha=0.5)\n",
        "    plt.title(f'Scatter Plot: {feature} vs Weekly_Sales', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Correlation matrix\n",
        "correlation_matrix = df[numerical_features].corr()\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Matrix of Numerical Features', fontsize=16)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gsGIaTROLnNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Identifying Outliers"
      ],
      "metadata": {
        "id": "vbDI6KR8F-RV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot(df, column):\n",
        "    plt.figure(figsize=(20,5))\n",
        "    plt.subplot(1,3,1)\n",
        "    sns.boxplot(data=df, x=column)\n",
        "    plt.title(f'Box Plot for {column}')\n",
        "\n",
        "    plt.subplot(1,3,2)\n",
        "    sns.histplot(data=df, x=column, kde=True, bins=50)\n",
        "    plt.title(f'Distribution Plot for {column}')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "LP55l82iF3H3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in ['Weekly_Sales', 'Temperature', 'Markdown_Total', 'Unemployment']:\n",
        "    plot(df, i)"
      ],
      "metadata": {
        "id": "dxCRUBpJF3Bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import skew\n",
        "\n",
        "# Assuming your data is in a DataFrame called 'df'\n",
        "columns_to_check = ['Weekly_Sales', 'Temperature', 'Fuel_Price', 'Markdown_Total', 'CPI', 'Unemployment']\n",
        "\n",
        "for column in columns_to_check:\n",
        "    skewness_value = skew(df[column])\n",
        "    print(f'Skewness for {column}: {skewness_value}')\n"
      ],
      "metadata": {
        "id": "LVVdIqFiF27B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a small constant to the original values before applying the log transformation\n",
        "import numpy as np\n",
        "\n",
        "small_const = 1\n",
        "df1 = df.copy()\n",
        "df1['Weekly_Sales_log'] = np.log(df1['Weekly_Sales'] + small_const)\n",
        "df1['Temperature_log'] = np.log(df1['Temperature'] + small_const)\n",
        "df1['MarkDown_Total_log'] = np.log(df1['Markdown_Total'] + small_const)\n",
        "df1['Unemployment_log'] = np.log(df1['Unemployment'] + small_const)\n",
        "\n",
        "# Display the transformed DataFrame\n",
        "print(df1)\n"
      ],
      "metadata": {
        "id": "sxhij4puF2vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# after log transformation the data reduced the skewness. [hist plot ]\n",
        "\n",
        "for i in ['Weekly_Sales_log','Temperature_log','MarkDown_Total_log','Unemployment_log']:\n",
        "     plot(df1, i)"
      ],
      "metadata": {
        "id": "JGw0H8gQHEe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import skew\n",
        "\n",
        "# List of transformed variables\n",
        "transformed_variables = ['Weekly_Sales_log', 'Temperature_log', 'MarkDown_Total_log', 'Unemployment_log']\n",
        "\n",
        "# Calculate skewness for each transformed variable\n",
        "skewness_results = {}\n",
        "for variable in transformed_variables:\n",
        "    # Handle NaN values by replacing them with 0\n",
        "    df1[variable] = df1[variable].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "    skewness = skew(df1[variable])\n",
        "    skewness_results[variable] = skewness\n",
        "    print(f'Skewness for {variable}: {skewness}')\n",
        "\n",
        "# View skewness results\n",
        "print(\"\\nSkewness Results:\")\n",
        "print(skewness_results)"
      ],
      "metadata": {
        "id": "RPEmk2_FHEZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Outliers Handling - Interquartile Range (IQR) method\n",
        "df2 = df1.copy()\n",
        "df2"
      ],
      "metadata": {
        "id": "EleDz1L-HESZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using IQR and clip() methods to handle the outliers and add a new column of dataframe\n",
        "\n",
        "def outlier(df, column):\n",
        "    iqr = df[column].quantile(0.75) - df[column].quantile(0.25)\n",
        "    upper_threshold = df[column].quantile(0.75) + (1.5*iqr)\n",
        "    lower_threshold = df[column].quantile(0.25) - (1.5*iqr)\n",
        "    df[column] = df[column].clip(lower_threshold, upper_threshold)"
      ],
      "metadata": {
        "id": "hadT_dSzHEHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (Ex: lower threshold = 5 and upper threshold = 20)\n",
        "# above upper threshold values (>20) are converted to upper threshold value (20) in features\n",
        "# below lower threshold values (<5)  are converted to lower threshold value (5)  in features\n",
        "\n",
        "outlier(df2, 'Weekly_Sales_log')\n",
        "outlier(df2, 'Temperature_log')\n",
        "outlier(df2, 'Unemployment_log')\n",
        "outlier(df2, 'MarkDown_Total_log')\n",
        "df2"
      ],
      "metadata": {
        "id": "5SIuZ640HhN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in ['Weekly_Sales_log','Temperature_log','MarkDown_Total_log','Unemployment_log']:\n",
        "     plot(df2, i)"
      ],
      "metadata": {
        "id": "CyqsFpvEHhI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary library\n",
        "from scipy.stats import skew\n",
        "\n",
        "# List of columns to check for skewness\n",
        "columns_to_check = ['Unemployment','Weekly_Sales_log', 'Temperature_log', 'MarkDown_Total_log', 'Unemployment_log']\n",
        "\n",
        "# Print skewness for each column\n",
        "for column in columns_to_check:\n",
        "    print(f\"Skewness for {column}: {df2[column].skew()}\")\n"
      ],
      "metadata": {
        "id": "jXBAstkDHg0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3 = df2.drop(columns=['Weekly_Sales','Temperature','Markdown_Total','Unemployment'])\n",
        "df3"
      ],
      "metadata": {
        "id": "rqBrZOt-H1ZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3.dtypes"
      ],
      "metadata": {
        "id": "mi8wD7oIH1Re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a new column 'Expected_Sales' to calculate the sum of 'MarkDown_Total_log' and 'Weekly_Sales_log'\n",
        "df4 = df3.copy()\n",
        "df4['Expected_Sales'] = df4['MarkDown_Total_log'] + df4['Weekly_Sales_log']\n",
        "\n",
        "# Display the DataFrame with the new column\n",
        "print(df4)\n"
      ],
      "metadata": {
        "id": "4kI5TvVIYm0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display summary statistics of the remaining columns\n",
        "summary_statistics = df4.describe()\n",
        "print(summary_statistics)"
      ],
      "metadata": {
        "id": "YuSxm14wGXqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the DataFrame with selected columns to a CSV file\n",
        "df4.to_csv('sales_prediction.csv', index=False)"
      ],
      "metadata": {
        "id": "PPeyKn-Qssxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fenzAeYUGhDe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}