{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8w1w3i1HG5u"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/sales_prediction.csv')"
      ],
      "metadata": {
        "id": "W-W_G0vtJkiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "kUub85WrKAOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "f0n5ydnaKn92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn import metrics\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor"
      ],
      "metadata": {
        "id": "g1eguCSQKnyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def machine_learning(df, algorithm):\n",
        "    # Specify features (X) and target variable (y)\n",
        "    X = df.drop('Weekly_Sales_log', axis=1)\n",
        "    y = df['Weekly_Sales_log']\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Dictionary to store results\n",
        "    results = {}\n",
        "\n",
        "    # Train and evaluate the selected algorithm\n",
        "    model = algorithm\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate performance metrics\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    # Store results in the dictionary\n",
        "    results['MAE'] = mae\n",
        "    results['MSE'] = mse\n",
        "    results['RMSE'] = rmse\n",
        "    results['R2'] = r2\n",
        "\n",
        "    return results\n",
        "\n",
        "# List of algorithms\n",
        "algorithms = [\n",
        "    DecisionTreeRegressor(random_state=42),\n",
        "    ExtraTreesRegressor(random_state=42),\n",
        "    RandomForestRegressor(random_state=42),\n",
        "    AdaBoostRegressor(random_state=42),\n",
        "    GradientBoostingRegressor(random_state=42)\n",
        "]\n",
        "\n",
        "# Iterate through algorithms and print results\n",
        "for algorithm in algorithms:\n",
        "    results = machine_learning(df, algorithm)\n",
        "    print(f'Model: {algorithm.__class__.__name__}')\n",
        "    print(f'Mean Absolute Error: {results[\"MAE\"]:.4f}')\n",
        "    print(f'Mean Squared Error: {results[\"MSE\"]:.4f}')\n",
        "    print(f'Root Mean Squared Error: {results[\"RMSE\"]:.4f}')\n",
        "    print(f'R^2 Score: {results[\"R2\"]:.4f}')\n",
        "    print('-' * 40)"
      ],
      "metadata": {
        "id": "eOmdMzXoKnfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the performance of regression algorithms using standard metrics such as\n",
        "# Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and R-squared (R2) score.\n",
        "# It takes time to train and evaluate the selected algorithm around 10-15 minutes.please wait till it completes."
      ],
      "metadata": {
        "id": "KlnYG_pCLulG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ExtraTreesRegressor and RandomForestRegressor perform the best among the models,\n",
        "# with lower MAE, MSE, and RMSE, indicating better prediction accuracy and precision.\n",
        "# They also have higher R2 scores, suggesting a better fit to the data."
      ],
      "metadata": {
        "id": "lYku49EELuab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XzXA3nXUMJkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# note: hyper parameter tuning is a process of selecting best parameters from set of parameters values  to increase the model performace\n",
        "# Grid search cv is the one of the method for hyper parameter tuning\n",
        "# it takes time around 2 hours to select the best parameters, please wait till it completes."
      ],
      "metadata": {
        "id": "UyncZalQMJdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the feature matrix (X) and target vector (y)\n",
        "x = df.drop(['Weekly_Sales_log'], axis=1)\n",
        "y = df['Weekly_Sales_log']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a reduced parameter grid\n",
        "param_grid_r = {\n",
        "    'max_depth': [5, 10, 15],\n",
        "    'min_samples_split': [5, 10, 15],\n",
        "    'min_samples_leaf': [2, 4, 6],\n",
        "    'max_features': ['sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search_r = GridSearchCV(estimator=RandomForestRegressor(),\n",
        "                             param_grid=param_grid_r,\n",
        "                             cv=5,\n",
        "                             n_jobs=-1)\n",
        "\n",
        "# Fit GridSearchCV to training data\n",
        "grid_search_r.fit(x_train, y_train)\n"
      ],
      "metadata": {
        "id": "uVcPh1wyMJUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_params = grid_search_r.best_params_\n",
        "\n",
        "score = grid_search_r.best_score_\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "\n",
        "print(\"Best Score:\", score)"
      ],
      "metadata": {
        "id": "su-wFtjIMJGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pass the parameters and check the accuracy for both training and testing & overfitting\n",
        "\n",
        "x = df.drop(columns=['Weekly_Sales_log'], axis=1)\n",
        "y = df['Weekly_Sales_log']\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)\n",
        "\n",
        "model = RandomForestRegressor(max_depth=15, max_features='log2', min_samples_leaf=4, min_samples_split=10).fit(x_train, y_train)\n",
        "y_pred_train = model.predict(x_train)\n",
        "y_pred_test = model.predict(x_test)\n",
        "\n",
        "r2_train = r2_score(y_train, y_pred_train)\n",
        "r2_test = r2_score(y_test, y_pred_test)\n",
        "r2_train, r2_test"
      ],
      "metadata": {
        "id": "NVlPG2qgsFwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predict the sales with hypertuning parameters and calculate the accuracy using metrics\n",
        "\n",
        "x = df.drop(columns=['Weekly_Sales_log'], axis=1)\n",
        "y = df['Weekly_Sales_log']\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)\n",
        "\n",
        "model = RandomForestRegressor(max_depth=15, max_features='log2', min_samples_leaf=4, min_samples_split=10).fit(x_train, y_train)\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "metrics_r = {'R2': r2,\n",
        "           'Mean Absolute Error': mae,\n",
        "           'Mean Squared Error': mse,\n",
        "           'Root Mean Squared Error': rmse}\n",
        "\n",
        "metrics_r"
      ],
      "metadata": {
        "id": "OBeZBm9qukud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "bQWJcxqBzfpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# manually pass the user input and predict the department wide sales for each store in the following year.\n",
        "user_data = np.array([[1,2,118221,93,0,3.882,211.096358,5,2,2010,3.768384,0.000000,2.208934,10.595510]])\n",
        "y_pred = model.predict(user_data)\n",
        "y_pred[0]"
      ],
      "metadata": {
        "id": "Wc6_eW-Ywmdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have feature names in a list (replace with your actual feature names)\n",
        "feature_names = ['Store', 'Type', 'Size', 'Dept', 'IsHoliday', 'Fuel_Price', 'CPI', 'Day', 'Month', 'Year', 'Temperature_log', 'MarkDown_Total_log', 'Unemployment_log','Expected_Sales']\n",
        "\n",
        "# Manually pass user input features with feature names\n",
        "user_data = pd.DataFrame([[1, 2, 118221, 93, 0, 3.882, 211.096358, 5, 2, 2010, 3.768384, 0.000000, 2.208934, 10.595510]], columns=feature_names)\n",
        "\n",
        "# Predict department-wide sales\n",
        "y_pred = model.predict(user_data)\n",
        "\n",
        "# Print or use the predicted sales\n",
        "print(\"Predicted Department-Wide Sales:\", y_pred[0])\n"
      ],
      "metadata": {
        "id": "M761UJ3x5xwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using Inverse Log Transformation to convert the value to original scale of the data (exp)\n",
        "np.exp(y_pred[0])"
      ],
      "metadata": {
        "id": "56wHm_Qv6NLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the regression model by using pickle\n",
        "import pickle\n",
        "with open('reg_model.pkl', 'wb') as f:\n",
        "    pickle.dump(model, f)"
      ],
      "metadata": {
        "id": "lC4OFEQZ6M9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load the pickle model\n",
        "with open('/content/reg_model.pkl', 'rb') as f:\n",
        "    model = pickle.load(f)\n",
        "\n",
        "# Your input data (replace this with your actual input data)\n",
        "user_data = np.array([[1, 2, 118221, 93, 0, 3.882, 211.096358, 5, 2, 2010, 3.768384, 0.000000, 2.208934, 10.595510]])\n",
        "\n",
        "# Feature names for your input data (replace this with actual feature names)\n",
        "feature_names = ['Store', 'Type', 'Size', 'Dept', 'IsHoliday', 'Fuel_Price', 'CPI', 'Day', 'Month', 'Year', 'Temperature_log', 'MarkDown_Total_log', 'Unemployment_log','Expected_Sales']\n",
        "\n",
        "# Create a DataFrame with the input data and feature names\n",
        "user_df = pd.DataFrame(user_data, columns=feature_names)\n",
        "\n",
        "# Make predictions by providing the input data (X) to the predict method\n",
        "y_pred = model.predict(user_df)\n",
        "\n",
        "# Assuming you're using log-transformed target values, exponentiate the prediction\n",
        "predicted_price = np.exp(y_pred[0])\n",
        "\n",
        "# Print the predicted selling price\n",
        "print(\"Predicted Department-Wide Sales:\", predicted_price)\n"
      ],
      "metadata": {
        "id": "94sgH5ogrZ5W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}